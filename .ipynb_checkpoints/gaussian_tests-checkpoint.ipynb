{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f28357bf-b301-462f-8823-959a4655eeed",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# General Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2272d16-7671-469f-9760-57ec2fe52a19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# standard imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab707f4-770d-4402-8c7c-3f315bddc9ef",
   "metadata": {},
   "source": [
    "Below block is library functions that will be removed once library is made available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9667eb23-c2f4-4325-a88b-9ce3189f040b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def squaredEuclideanDistance(X, Y):\n",
    "    \"\"\" Compute matrix with pairwise squared Euclidean distances\n",
    "    between the rows of X and the rows of Y\n",
    "                 D_{i,j} = ||x_i - y_j ||^2\n",
    "    Args:\n",
    "      X: A tensor with N as the first dim.\n",
    "      Y: a tensor with M as the first dim.\n",
    "      the remaining dimensions of X and Y must match\n",
    "    Returns:\n",
    "      D: a N x M array where\n",
    "    \"\"\"\n",
    "    G12 = torch.matmul(X, Y.t())\n",
    "    G11 = torch.sum(torch.square(X), axis=1, keepdim=True)\n",
    "    G22 = torch.sum(torch.square(Y), axis=1, keepdim=True)\n",
    "    D = G11 - G12 * torch.tensor(2, dtype=X.dtype) + G22.t()\n",
    "    return D\n",
    "\n",
    "\n",
    "def generalizedInformationPotential(K, alpha):\n",
    "    \"\"\"Computes the generalized information\n",
    "    potential of order alpha\n",
    "          GIP_alpha(K) = trace(K_^alpha),\n",
    "    where K^alpha is a matrix raised to the alpha power.\n",
    "    K_ is normalized as K_ = K / trace(K), such that\n",
    "          trace(K_) = 1.\n",
    "    Args:\n",
    "      K: (N x N) Gram matrix.\n",
    "      alpha: order of the entropy.\n",
    "    Returns:\n",
    "      GIP: generalized information potential of alpha order.\n",
    "    \"\"\"\n",
    "    ek = torch.linalg.eigvalsh(K)\n",
    "    mk = torch.gt(ek, 0.0)\n",
    "    mek = ek[mk]\n",
    "    mek = mek / torch.sum(mek)\n",
    "    GIP = torch.sum(torch.exp(alpha * torch.log(mek)))\n",
    "    return GIP\n",
    "\n",
    "def matrixAlphaEntropy(K, alpha):\n",
    "    \"\"\"Computes the matrix based alpha-entropy\n",
    "    based on the spectrum of K\n",
    "        H_alpha(A) = (1/(1-alpha))log(trace(A^alpha)),\n",
    "    where A^alpha is the matrix power of alpha (A is normalized).\n",
    "    Args:\n",
    "      A: (N x N) Gram matrix.\n",
    "      alpha: order of the entropy.\n",
    "    Returns:\n",
    "      H: alpha entropy\n",
    "    \"\"\"\n",
    "    ##ccompute generalized information Potential\n",
    "    GIP = generalizedInformationPotential(K, alpha)\n",
    "    H = (1.0 / (1.0 - alpha)) * np.log(GIP)\n",
    "    return H\n",
    "\n",
    "\n",
    "def matrixAlphaJointEntropy(K_list, alpha):\n",
    "    \"\"\"Computes the matrix based alpha joint-entropy\n",
    "    based on the spectrum of K\n",
    "        H_alpha(K) = (1/(1-alpha))log(trace(K^alpha)),\n",
    "    where K^alpha is the matrix power of K (K is normalized).\n",
    "    Args:\n",
    "      K_list: a list of (N x N) Gram matrices.\n",
    "      alpha: order of the entropy.\n",
    "      normalize: Boolean (default True)\n",
    "    Returns:\n",
    "      H: alpha joint entropy\n",
    "    \"\"\"\n",
    "    K = reduce(lambda x, y: x * y, K_list)\n",
    "    return matrixAlphaEntropy(K, alpha)\n",
    "\n",
    "\n",
    "def gaussianKernel(X, Y, sigma):\n",
    "    \"\"\" Compute the Gram matrix using a Gaussian kernel.\n",
    "    where K(i, j) = exp( - (1/(2 * sigma^2)) * || X[i,::] - Y[j, ::] ||^2 )\n",
    "    Args:\n",
    "      X: A tensor with N as the first dim.\n",
    "      Y: a tensor with M as the first dim.\n",
    "      sigma: scale parameter (scalar)\n",
    "    Returns:\n",
    "      K: a N x M gram matrix\n",
    "    \"\"\"\n",
    "    D = squaredEuclideanDistance(X, Y)\n",
    "    return torch.exp(-D / (2.0 * sigma ** 2))\n",
    "\n",
    "def mmd(x, y, sigma):\n",
    "    # compare kernel MMD paper and code:\n",
    "    # A. Gretton et al.: A kernel two-sample test, JMLR 13 (2012)\n",
    "    # http://www.gatsby.ucl.ac.uk/~gretton/mmd/mmd.htm\n",
    "    # x shape [n, d] y shape [m, d]\n",
    "    # n_perm number of bootstrap permutations to get p-value, pass none to not get p-value\n",
    "    n, d = x.shape\n",
    "    m, d2 = y.shape\n",
    "    assert d == d2\n",
    "    xy = torch.cat([x.detach(), y.detach()], dim=0)\n",
    "    dists = torch.cdist(xy, xy, p=2.0)\n",
    "    # we are a bit sloppy here as we just keep the diagonal and everything twice\n",
    "    # note that sigma should be squared in the RBF to match the Gretton et al heuristic\n",
    "    k = torch.exp((-1 / (2 * sigma ** 2)) * dists ** 2)\n",
    "    k_x = k[:n, :n]\n",
    "    k_y = k[n:, n:]\n",
    "    k_xy = k[:n, n:]\n",
    "    # The diagonals are always 1 (up to numerical error, this is (3) in Gretton et al.)\n",
    "    # note that their code uses the biased (and differently scaled mmd)\n",
    "    # mmd unbiased does not sum the diagonal terms.\n",
    "    mmd = (k_x.sum() - n) / (n * (n - 1)) + (k_y.sum() - m) / (m * (m - 1)) - 2 * k_xy.sum() / (n * m)\n",
    "    return mmd\n",
    "\n",
    "\n",
    "def divergenceRJ(X, Y, sigma, alpha):\n",
    "    n1 = X.shape[0]\n",
    "    n2 = Y.shape[0]\n",
    "    l = torch.ones(n1 + n2, dtype=torch.long)\n",
    "    l[n1:] = 0\n",
    "\n",
    "    XY = torch.cat((X, Y))\n",
    "    K = gaussianKernel(XY, XY, sigma)\n",
    "\n",
    "    L = torch.nn.functional.one_hot(l).type(X.dtype)\n",
    "    Kl = torch.matmul(L, L.t())\n",
    "\n",
    "    Hxy = matrixAlphaEntropy(K, alpha=alpha)\n",
    "    Hj = matrixAlphaJointEntropy([K, Kl], alpha=alpha)\n",
    "    Hl = matrixAlphaEntropy(Kl, alpha)\n",
    "    divergence = Hxy + Hl - Hj\n",
    "    return divergence\n",
    "\n",
    "def gaussian_data(mu1, mu2, cov, n):\n",
    "    X = torch.from_numpy(np.random.multivariate_normal(mu1, cov, n))\n",
    "    Y = torch.from_numpy(np.random.multivariate_normal(mu1, cov, n))\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc030db-f2e5-4867-b6bb-7d6ef9461a66",
   "metadata": {},
   "source": [
    "# Experimental Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dbc8cc-020d-4882-b25a-51da35b12e08",
   "metadata": {},
   "source": [
    "Configuration for experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4612425e-5604-44d9-b2a2-46e01fc6e91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphaList = [1.01, 2.0, 5.0]    # list of different alphas for the jensen-reyni divergence\n",
    "\n",
    "numSamples = 100                 # number of data samples\n",
    "numDimensions = 10              # number of dimensions to try\n",
    "numDiffs = 10                    # number of differences to try\n",
    "numRepetitions = 10             # number of times to repeat the experiment\n",
    "permTestSize = 50               # size of permutation test\n",
    "significance = 0.05             # significance level\n",
    "\n",
    "# The following configs match what was done in the paper. \n",
    "# However, they were used on a cluster and are therefore not practical for sequential code\n",
    "# numSamples = 250                 # number of data samples\n",
    "# numDimensions = 10              # number of dimensions to try\n",
    "# numDiffs = 20                    # number of differences to try\n",
    "# numRepetitions = 100             # number of times to repeat the experiment\n",
    "# permTestSize = 500               # size of permutation test\n",
    "# significance = 0.05             # significance level\n",
    "\n",
    "\n",
    "dimLowerBound, dimUpperBound = 2, 2500.01\n",
    "dimensionList = np.logspace(np.log10(dimLowerBound), np.log10(dimUpperBound), numDimensions, dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d125a62d-c68d-4e41-914c-f287ca060b22",
   "metadata": {},
   "source": [
    "Functions that are used in both mean and variance test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09aa2f64-03a2-4872-92d2-51d07275b28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mmd_permutation_test(x, y, sigma, significance, rep):\n",
    "    N_X = len(x)\n",
    "    N_Y = len(y)\n",
    "    xy = torch.cat([x, y], dim=0).double()\n",
    "    mmds = []\n",
    "    \n",
    "    for i in range(rep):\n",
    "        xy = xy[torch.randperm(len(xy))]\n",
    "        mmds.append(mmd(xy[:N_X, :], xy[N_X:, :], sigma).item())\n",
    "        \n",
    "    mmds = torch.tensor(mmds)\n",
    "    thr_mmd = torch.quantile(mmds, (1 - significance))\n",
    "    return thr_mmd\n",
    "\n",
    "def jrd_permutation_test(x, y, sigma, alpha, significance, rep):\n",
    "    N_X = len(x)\n",
    "    N_Y = len(y)\n",
    "    xy = torch.cat([x, y], dim=0).double()\n",
    "    jrd = []\n",
    "    \n",
    "    for i in range(rep):\n",
    "        xy = xy[torch.randperm(len(xy))]\n",
    "        jrd.append(divergenceRJ(xy[:N_X, :], xy[N_X:, :], sigma, alpha).item())\n",
    "        \n",
    "    jrd = torch.tensor(jrd)\n",
    "    thr_jrd = torch.quantile(jrd, (1 - significance))\n",
    "    return thr_jrd\n",
    "\n",
    "def run_experiment(iterableDiffs, dataGenFunction):\n",
    "    # storage for results\n",
    "    mmd_intermediateResults = np.zeros([numRepetitions, numDimensions, len(iterableDiffs)])\n",
    "    jrd_intermediateResults = np.zeros([len(alphaList), numRepetitions, numDimensions, len(iterableDiffs)])\n",
    "\n",
    "    # main experiment loop\n",
    "    for repIdx in range(numRepetitions):\n",
    "        for dIdx, dim in enumerate(dimensionList):\n",
    "            for itIdx, diff in enumerate(iterableDiffs):\n",
    "                # construct two distributions\n",
    "                X, Y = dataGenFunction(dim, diff)\n",
    "                \n",
    "                # create mixture\n",
    "                XY = torch.from_numpy(np.concatenate([X, Y]))\n",
    "\n",
    "                # calculate euclidean distances and kernel bandwidth\n",
    "                dists = squaredEuclideanDistance(XY, XY)\n",
    "                sigma = torch.sqrt(torch.sum(dists) / ( ((numSamples*2)**2 - (numSamples*2)) * 2 ))\n",
    "\n",
    "                # collect MMD baeline and permutation test\n",
    "                mmd_ = mmd(X, Y, sigma)\n",
    "                thr_mmd = mmd_permutation_test(X, Y, sigma, significance, permTestSize)\n",
    "                if thr_mmd < mmd_:\n",
    "                    mmd_intermediateResults[repIdx, dIdx, itIdx] = 1\n",
    "\n",
    "                # collect JRD baeline and permutation test for different alphas\n",
    "                for alphaIdx, alpha in enumerate(alphaList):\n",
    "                    jrd_ = divergenceRJ(X, Y, sigma, alpha)\n",
    "                    thr_jrd = jrd_permutation_test(X, Y, sigma, alpha, significance, permTestSize)\n",
    "                    if thr_jrd < jrd_:\n",
    "                        jrd_intermediateResults[alphaIdx, repIdx, dIdx, itIdx] = 1\n",
    "                        \n",
    "    return mmd_intermediateResults, jrd_intermediateResults"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a69b9d-790c-4f92-9979-a92e48280c21",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Mean Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55ee0de-d5d4-47b4-b6a0-a1dbfedc6c82",
   "metadata": {},
   "source": [
    "Gaussian means test from Gretton 2012 paper. The two distributions have the same variance, but increasingly differing means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e80015-6449-4dca-a523-48505a1178f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two distributions with unit variance, and with given dimension and difference in means\n",
    "def generate_mean_test_distributions(dim, meanDiff):\n",
    "    identity = np.eye(dim)\n",
    "        \n",
    "    xMean = np.zeros(dim)\n",
    "    yMean = np.full(dim, meanDiff / (dim**0.5))\n",
    "    X = torch.from_numpy(np.random.multivariate_normal(mean=xMean, cov=identity, size=numSamples))\n",
    "    Y = torch.from_numpy(np.random.multivariate_normal(mean=yMean, cov=identity, size=numSamples))\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "\n",
    "meanDiffs = np.logspace(np.log10(0.05), np.log10(50), numDiffs)\n",
    "meanResults_mmd, meanResults_jrd = run_experiment(meanDiffs, generate_mean_test_distributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e217079f-922f-4e03-b65f-2e1e634e5972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute averages\n",
    "compactResults_mmd = np.mean(meanResults_mmd[:,:,:], axis=0)\n",
    "averaged_mmd = np.mean(compactResults_mmd, axis=1)\n",
    "plt.plot(dimensionList, averaged_mmd, label='mmd')\n",
    "\n",
    "for alphaIdx, alpha in enumerate(alphaList):\n",
    "    compactResults_jrd = np.mean(meanResults_jrd[alphaIdx,:,:,:], axis=0)\n",
    "    averaged_jrd = np.mean(compactResults_jrd, axis=1)\n",
    "    plt.plot(dimensionList, averaged_jrd, label='jrd %.2f' % alpha)\n",
    "\n",
    "plt.title(\"Performance of Divergence on Gaussians with Different Means\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Dimensionality\")\n",
    "plt.ylabel(\"Percentage of Rejection of Null Hypothesis\")\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b428e5cc-94aa-4f87-a43c-ffbf194f9447",
   "metadata": {},
   "source": [
    "# Variance Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d1d707-5126-4c4a-9396-c70c109306dc",
   "metadata": {},
   "source": [
    "Gaussian variance test from Gretton 2012 paper. The two distributions have the same mean, but increasingly differing variances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b1790c-20d2-48a7-8605-31ec720ff7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two distributions with zero mean, and with given dimension and difference in variances\n",
    "def generate_variance_test_distributions(dim, var):\n",
    "    identity = np.eye(dim)\n",
    "    zeroMean = np.zeros(dim)\n",
    "    \n",
    "    X = torch.from_numpy(np.random.multivariate_normal(zeroMean, cov=identity, size=numSamples))\n",
    "    Y = torch.from_numpy(np.random.multivariate_normal(zeroMean, cov=var * identity, size=numSamples))\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "variances = np.logspace(0.01, 1, numDiffs)\n",
    "varResults_mmd, varResults_jrd = run_experiment(variances, generate_variance_test_distributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e285a66-ca32-402c-89f5-1842a27efdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute averages\n",
    "compactResults_mmd = np.mean(varResults_mmd[:,:,:], axis=0)\n",
    "averaged_mmd = np.mean(compactResults_mmd, axis=1)\n",
    "plt.plot(dimensionList, averaged_mmd, label='mmd')\n",
    "\n",
    "for alphaIdx, alpha in enumerate(alphaList):\n",
    "    compactResults_jrd = np.mean(varResults_jrd[alphaIdx,:,:,:], axis=0)\n",
    "    averaged_jrd = np.mean(compactResults_jrd, axis=1)\n",
    "    plt.plot(dimensionList, averaged_jrd, label='jrd %.2f' % alpha)\n",
    "\n",
    "plt.title(\"Performance of Divergence on Gaussians with Different Variances\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Dimensionality\")\n",
    "plt.ylabel(\"Percentage of Rejection of Null Hypothesis\")\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35466de-f91c-4564-bcec-45fb02681fe9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
