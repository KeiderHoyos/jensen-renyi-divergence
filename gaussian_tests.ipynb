{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f28357bf-b301-462f-8823-959a4655eeed",
   "metadata": {
    "tags": []
   },
   "source": [
    "# General Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2272d16-7671-469f-9760-57ec2fe52a19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# standard imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "\n",
    "# imports from representation-itl library\n",
    "import repitl.kernel_utils as ku\n",
    "import repitl.matrix_itl as itl\n",
    "import repitl.divergences as div"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc030db-f2e5-4867-b6bb-7d6ef9461a66",
   "metadata": {},
   "source": [
    "# Experimental Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dbc8cc-020d-4882-b25a-51da35b12e08",
   "metadata": {},
   "source": [
    "Configuration for experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4612425e-5604-44d9-b2a2-46e01fc6e91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphaList = [1.01, 2.0, 5.0]    # list of different alphas for the jensen-reyni divergence\n",
    "\n",
    "numSamples = 100                 # number of data samples\n",
    "numDimensions = 10              # number of dimensions to try\n",
    "numDiffs = 10                    # number of differences to try\n",
    "numRepetitions = 10             # number of times to repeat the experiment\n",
    "permTestSize = 50               # size of permutation test\n",
    "significance = 0.05             # significance level\n",
    "\n",
    "# The following configs match what was done in the paper. \n",
    "# However, they were used on a cluster and are therefore not practical for sequential code\n",
    "# numSamples = 250                 # number of data samples\n",
    "# numDimensions = 10              # number of dimensions to try\n",
    "# numDiffs = 20                    # number of differences to try\n",
    "# numRepetitions = 100             # number of times to repeat the experiment\n",
    "# permTestSize = 500               # size of permutation test\n",
    "# significance = 0.05             # significance level\n",
    "\n",
    "\n",
    "dimLowerBound, dimUpperBound = 2, 2500.01\n",
    "dimensionList = np.logspace(np.log10(dimLowerBound), np.log10(dimUpperBound), numDimensions, dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d125a62d-c68d-4e41-914c-f287ca060b22",
   "metadata": {},
   "source": [
    "Functions that are used in both mean and variance test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09aa2f64-03a2-4872-92d2-51d07275b28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mmd(x, y, sigma):\n",
    "    # compare kernel MMD paper and code:\n",
    "    # A. Gretton et al.: A kernel two-sample test, JMLR 13 (2012)\n",
    "    # http://www.gatsby.ucl.ac.uk/~gretton/mmd/mmd.htm\n",
    "    # x shape [n, d] y shape [m, d]\n",
    "    # n_perm number of bootstrap permutations to get p-value, pass none to not get p-value\n",
    "    n, d = x.shape\n",
    "    m, d2 = y.shape\n",
    "    assert d == d2\n",
    "    xy = torch.cat([x.detach(), y.detach()], dim=0)\n",
    "    dists = torch.cdist(xy, xy, p=2.0)\n",
    "    # we are a bit sloppy here as we just keep the diagonal and everything twice\n",
    "    # note that sigma should be squared in the RBF to match the Gretton et al heuristic\n",
    "    k = torch.exp((-1 / (2 * sigma ** 2)) * dists ** 2)\n",
    "    k_x = k[:n, :n]\n",
    "    k_y = k[n:, n:]\n",
    "    k_xy = k[:n, n:]\n",
    "    # The diagonals are always 1 (up to numerical error, this is (3) in Gretton et al.)\n",
    "    # note that their code uses the biased (and differently scaled mmd)\n",
    "    # mmd unbiased does not sum the diagonal terms.\n",
    "    mmd = (k_x.sum() - n) / (n * (n - 1)) + (k_y.sum() - m) / (m * (m - 1)) - 2 * k_xy.sum() / (n * m)\n",
    "    return mmd\n",
    "\n",
    "\n",
    "def mmd_permutation_test(x, y, sigma, significance, rep):\n",
    "    \"\"\"\n",
    "    Performs a permutation test on the MMD method\n",
    "    \"\"\"\n",
    "    \n",
    "    N_X = len(x)\n",
    "    N_Y = len(y)\n",
    "    xy = torch.cat([x, y], dim=0).double()\n",
    "    mmds = []\n",
    "    \n",
    "    for i in range(rep):\n",
    "        xy = xy[torch.randperm(len(xy))]\n",
    "        mmds.append(mmd(xy[:N_X, :], xy[N_X:, :], sigma).item())\n",
    "        \n",
    "    mmds = torch.tensor(mmds)\n",
    "    thr_mmd = torch.quantile(mmds, (1 - significance))\n",
    "    return thr_mmd\n",
    "\n",
    "def jrd_permutation_test(x, y, sigma, alpha, significance, rep):\n",
    "    \"\"\"\n",
    "    Performs a permutation test on the JRD method with a given alpha\n",
    "    \"\"\"\n",
    "    \n",
    "    N_X = len(x)\n",
    "    N_Y = len(y)\n",
    "    xy = torch.cat([x, y], dim=0).double()\n",
    "    jrd = []\n",
    "    \n",
    "    for i in range(rep):\n",
    "        xy = xy[torch.randperm(len(xy))]\n",
    "        jrd.append(div.divergenceJR(xy[:N_X, :], xy[N_X:, :], sigma, alpha).item())\n",
    "        \n",
    "    jrd = torch.tensor(jrd)\n",
    "    thr_jrd = torch.quantile(jrd, (1 - significance))\n",
    "    return thr_jrd\n",
    "\n",
    "def run_experiment(iterableDiffs, dataGenFunction):\n",
    "    \"\"\"\n",
    "    Generalized form of the mean/variance experiment\n",
    "    \n",
    "    Inputs:\n",
    "        dataGenFunction: A function that creates gaussian data with a specific mean or variance.\n",
    "                         This data generation function is specific to the actual experiment, such as means differences experiment.\n",
    "                         \n",
    "        iterableDiffs: A list of differnet values to iterate over. Can be differences in means or variances.\n",
    "                       Items inside are passed to the dataGenFunction to create data\n",
    "    \"\"\"\n",
    "    \n",
    "    # storage for results\n",
    "    mmd_intermediateResults = np.zeros([numRepetitions, numDimensions, len(iterableDiffs)])\n",
    "    jrd_intermediateResults = np.zeros([len(alphaList), numRepetitions, numDimensions, len(iterableDiffs)])\n",
    "\n",
    "    # main experiment loop\n",
    "    for repIdx in range(numRepetitions):\n",
    "        for dIdx, dim in enumerate(dimensionList):\n",
    "            for itIdx, diff in enumerate(iterableDiffs):\n",
    "                # construct two distributions\n",
    "                X, Y = dataGenFunction(dim, diff)\n",
    "                \n",
    "                # create mixture\n",
    "                XY = torch.from_numpy(np.concatenate([X, Y]))\n",
    "\n",
    "                # calculate euclidean distances and kernel bandwidth\n",
    "                dists = ku.squaredEuclideanDistance(XY, XY)\n",
    "                sigma = torch.sqrt(torch.sum(dists) / ( ((numSamples*2)**2 - (numSamples*2)) * 2 ))\n",
    "\n",
    "                # collect MMD baeline and permutation test\n",
    "                mmd_ = mmd(X, Y, sigma)\n",
    "                thr_mmd = mmd_permutation_test(X, Y, sigma, significance, permTestSize)\n",
    "                if thr_mmd < mmd_:\n",
    "                    mmd_intermediateResults[repIdx, dIdx, itIdx] = 1\n",
    "\n",
    "                # collect JRD baeline and permutation test for different alphas\n",
    "                for alphaIdx, alpha in enumerate(alphaList):\n",
    "                    jrd_ = div.divergenceJR(X, Y, sigma, alpha)\n",
    "                    thr_jrd = jrd_permutation_test(X, Y, sigma, alpha, significance, permTestSize)\n",
    "                    if thr_jrd < jrd_:\n",
    "                        jrd_intermediateResults[alphaIdx, repIdx, dIdx, itIdx] = 1\n",
    "                        \n",
    "    return mmd_intermediateResults, jrd_intermediateResults"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a69b9d-790c-4f92-9979-a92e48280c21",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Mean Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55ee0de-d5d4-47b4-b6a0-a1dbfedc6c82",
   "metadata": {},
   "source": [
    "Gaussian means test from Gretton 2012 paper. The two distributions have the same variance, but increasingly differing means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8e80015-6449-4dca-a523-48505a1178f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_339366/131157623.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mmeanDiffs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumDiffs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mmeanResults_mmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeanResults_jrd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeanDiffs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerate_mean_test_distributions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_339366/1436962438.py\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(iterableDiffs, dataGenFunction)\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mitIdx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiff\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterableDiffs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0;31m# construct two distributions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataGenFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;31m# create mixture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_339366/131157623.py\u001b[0m in \u001b[0;36mgenerate_mean_test_distributions\u001b[0;34m(dim, meanDiff)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mxMean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0myMean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeanDiff\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultivariate_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxMean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumSamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultivariate_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0myMean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumSamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.multivariate_normal\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msvd\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36msvd\u001b[0;34m(a, full_matrices, compute_uv, hermitian)\u001b[0m\n\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m         \u001b[0msignature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'D->DdD'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'd->ddd'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1660\u001b[0;31m         \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgufunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1661\u001b[0m         \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1662\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_realType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# create two distributions with unit variance, and with given dimension and difference in means\n",
    "def generate_mean_test_distributions(dim, meanDiff):\n",
    "    identity = np.eye(dim)\n",
    "        \n",
    "    xMean = np.zeros(dim)\n",
    "    yMean = np.full(dim, meanDiff / (dim**0.5))\n",
    "    X = torch.from_numpy(np.random.multivariate_normal(mean=xMean, cov=identity, size=numSamples))\n",
    "    Y = torch.from_numpy(np.random.multivariate_normal(mean=yMean, cov=identity, size=numSamples))\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "\n",
    "meanDiffs = np.logspace(np.log10(0.05), np.log10(50), numDiffs)\n",
    "meanResults_mmd, meanResults_jrd = run_experiment(meanDiffs, generate_mean_test_distributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e217079f-922f-4e03-b65f-2e1e634e5972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute averages\n",
    "compactResults_mmd = np.mean(meanResults_mmd[:,:,:], axis=0)\n",
    "averaged_mmd = np.mean(compactResults_mmd, axis=1)\n",
    "plt.plot(dimensionList, averaged_mmd, label='mmd')\n",
    "\n",
    "for alphaIdx, alpha in enumerate(alphaList):\n",
    "    compactResults_jrd = np.mean(meanResults_jrd[alphaIdx,:,:,:], axis=0)\n",
    "    averaged_jrd = np.mean(compactResults_jrd, axis=1)\n",
    "    plt.plot(dimensionList, averaged_jrd, label='jrd %.2f' % alpha)\n",
    "\n",
    "plt.title(\"Performance of Divergence on Gaussians with Different Means\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Dimensionality\")\n",
    "plt.ylabel(\"Percentage of Rejection of Null Hypothesis\")\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b428e5cc-94aa-4f87-a43c-ffbf194f9447",
   "metadata": {},
   "source": [
    "# Variance Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d1d707-5126-4c4a-9396-c70c109306dc",
   "metadata": {},
   "source": [
    "Gaussian variance test from Gretton 2012 paper. The two distributions have the same mean, but increasingly differing variances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b1790c-20d2-48a7-8605-31ec720ff7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two distributions with zero mean, and with given dimension and difference in variances\n",
    "def generate_variance_test_distributions(dim, var):\n",
    "    identity = np.eye(dim)\n",
    "    zeroMean = np.zeros(dim)\n",
    "    \n",
    "    X = torch.from_numpy(np.random.multivariate_normal(zeroMean, cov=identity, size=numSamples))\n",
    "    Y = torch.from_numpy(np.random.multivariate_normal(zeroMean, cov=var * identity, size=numSamples))\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "variances = np.logspace(0.01, 1, numDiffs)\n",
    "varResults_mmd, varResults_jrd = run_experiment(variances, generate_variance_test_distributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e285a66-ca32-402c-89f5-1842a27efdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute averages\n",
    "compactResults_mmd = np.mean(varResults_mmd[:,:,:], axis=0)\n",
    "averaged_mmd = np.mean(compactResults_mmd, axis=1)\n",
    "plt.plot(dimensionList, averaged_mmd, label='mmd')\n",
    "\n",
    "for alphaIdx, alpha in enumerate(alphaList):\n",
    "    compactResults_jrd = np.mean(varResults_jrd[alphaIdx,:,:,:], axis=0)\n",
    "    averaged_jrd = np.mean(compactResults_jrd, axis=1)\n",
    "    plt.plot(dimensionList, averaged_jrd, label='jrd %.2f' % alpha)\n",
    "\n",
    "plt.title(\"Performance of Divergence on Gaussians with Different Variances\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Dimensionality\")\n",
    "plt.ylabel(\"Percentage of Rejection of Null Hypothesis\")\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
